{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization (MF) Recommender Example\n",
    "Demonstrates matrix factorization with MXNet on the [MovieLens 100k](http://grouplens.org/datasets/movielens/100k/) dataset. We perform **collaborative filtering**, where the recommendations are based on previous rating of users.<br/>\n",
    "Citation: <br/>\n",
    "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:\n",
    "History and Context. ACM Transactions on Interactive Intelligent\n",
    "Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.\n",
    "DOI=http://dx.doi.org/10.1145/2827872\n",
    "\n",
    "We are trying to learn embeddings for users and movies, based on user partial ratings of movies, to estimate future movie ratings\n",
    "\n",
    "![](https://i.imgur.com/twyWChh.png)\n",
    "\n",
    "\n",
    "For more deep learning based architecture for recommendation, refer to this survey: [Deep Learning based Recommender System: A Survey and New Perspectives](https://arxiv.org/pdf/1707.07435.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matrix_fact import train\n",
    "from movielens_data import get_dataset, max_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = [mx.gpu(0)] if len(mx.test_utils.list_gpus()) > 0 else [mx.cpu()]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_dataset()\n",
    "max_user, max_item = max_id('./ml-100k/u.data')\n",
    "(max_user, max_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(train_dataset, shuffle=True, last_batch='rollover',\n",
    "                                   batch_size=batch_size, num_workers=0)\n",
    "test_data = gluon.data.DataLoader(test_dataset, shuffle=True, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, item, score in test_data:\n",
    "    print(user[0], item[0], score[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.DataFrame([d for d in train_dataset], columns=['user', 'item', 'score'])\n",
    "testdf = pd.DataFrame([d for d in test_dataset], columns=['user', 'item', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 1: avg training rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['pred'] = traindf['score'].mean()\n",
    "print(np.sqrt(np.mean(np.power(np.abs(testdf['pred'] - testdf['score']), 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline 2: avg of avg train user rating and avg train item rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useravg = (traindf[['user', 'score']].groupby('user')\n",
    "           .mean().reset_index().rename(columns={'score': 'useravg'}))\n",
    "\n",
    "itemavg = (traindf[['item', 'score']].groupby('item')\n",
    "           .mean().reset_index().rename(columns={'score': 'itemavg'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temptest = pd.merge(testdf, useravg, on='user')\n",
    "temptest = pd.merge(temptest, itemavg, on='item')\n",
    "temptest['superbase'] = 0.5*(temptest['useravg'] + temptest['itemavg'])\n",
    "\n",
    "print('avg user rating: ')\n",
    "print(np.sqrt(np.mean(np.power(np.abs(temptest['useravg'] - temptest['score']), 2))))\n",
    "print()\n",
    "\n",
    "print('avg item rating: ')\n",
    "print(np.sqrt(np.mean(np.power(np.abs(temptest['itemavg'] - temptest['score']), 2))))\n",
    "print()\n",
    "\n",
    "print('avg of avg user and item ratings: ')\n",
    "print(np.sqrt(np.mean(np.power(np.abs(temptest['superbase'] - temptest['score']), 2))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMatrixFactorization(gluon.HybridBlock):\n",
    "    \n",
    "    def __init__(self, k, max_user=max_user, max_item=max_item):\n",
    "        super(LinearMatrixFactorization, self).__init__(prefix='linearMF_')\n",
    "        \n",
    "        # user feature lookup\n",
    "        with self.name_scope():\n",
    "            self.user_embedding = gluon.nn.Embedding(input_dim=max_user, output_dim = k, prefix='emb_user_') \n",
    "\n",
    "            # item feature lookup\n",
    "            self.item_embedding = gluon.nn.Embedding(input_dim=max_item, output_dim = k, prefix='emb_item_') \n",
    "    \n",
    "    def hybrid_forward(self, F, user, item):\n",
    "        user_embeddings = self.user_embedding(user).relu()\n",
    "        items_embeddings = self.item_embedding(item).relu()\n",
    "        \n",
    "        # predict by the inner product, which is elementwise product and then sum\n",
    "        pred = (user_embeddings * items_embeddings).sum(axis=1)\n",
    "        \n",
    "        return pred.flatten()\n",
    "\n",
    "net1 = LinearMatrixFactorization(64)\n",
    "net1.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "mx.viz.plot_network(net1(mx.sym.var('user'), mx.sym.var('item')), node_attrs={\"fixedsize\":\"false\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1.summary(user.as_in_context(ctx[0]), item.as_in_context(ctx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses_1 = train(net1, train_data, test_data, epochs=15, learning_rate=1, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer used for training and hyper-parameter influence greatly how fast the model converge.\n",
    "We can try with the [Adam optimizer](https://arxiv.org/abs/1412.6980) which will often converge much faster than SGD without momentum as we used before.  You should see this model over-fitting quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = LinearMatrixFactorization(64)\n",
    "net1.initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1_adam = train(net1, train_data, test_data, epochs=15, optimizer='adam', learning_rate=0.01, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = nd.dot(net1.user_embedding.weight.data(ctx=ctx[0]), net1.item_embedding.weight.data(ctx=ctx[0]).T).asnumpy()\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print the recommendation matrix\n",
    "# And the top 5 movies in several categories\n",
    "\n",
    "def evaluate_embeddings(ratings):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.xlabel('items')\n",
    "    plt.ylabel('users')\n",
    "    plt.title('Users estimated ratings of items sorted by mean ratings across users')\n",
    "    im = plt.imshow(((ratings[:, ratings.mean(axis=0).argsort()[::-1]])))\n",
    "    cb = plt.colorbar(im,fraction=0.026, pad=0.04, label=\"score\")\n",
    "    \n",
    "    top_5_movies = ratings.mean(axis=0).argsort()[::-1][:5] # Highest mean projected rating\n",
    "    worst_5_movies = ratings.mean(axis=0).argsort()[:5] # Lowest mean projected rating\n",
    "    top_5_controversial = ratings.std(axis=0).argsort()[::-1][:5] # With most variance\n",
    "    \n",
    "    with open('ml-100k/u.item', 'rb') as f:\n",
    "        movies = f.readlines()\n",
    "        \n",
    "    print(\"Top 5 movies:\")\n",
    "    for movie in top_5_movies:\n",
    "        print(\"{}, average rating {:.2f}\".format(str(movies[int(movie)-1]).split(\"|\")[1], ratings.mean(axis=0)[movie]))\n",
    "    print(\"\\nWorst 5 movies:\")\n",
    "    for movie in worst_5_movies:\n",
    "        print(\"{}, average rating {:.2f}\".format(str(movies[int(movie)-1]).split(\"|\")[1], ratings.mean(axis=0)[movie]))\n",
    "    print(\"\\n5 most controversial movies:\")\n",
    "    for movie in top_5_controversial:\n",
    "        print(\"{}, average rating {:.2f}\".format(str(movies[int(movie)-1]).split(\"|\")[1], ratings.mean(axis=0)[movie]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_embeddings(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some movies tend to be widely recommended or not recommended, whilst some other have more variance in their predicted score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (non-linear) Matrix Factorization\n",
    "\n",
    "We don't have to limit ourselves to the weights of the linear embedding layer for our user or item embeddings. We can have a more complex pipeline combining fully connected layers and non-linear activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class MLPMatrixFactorization(gluon.HybridBlock):\n",
    "    \n",
    "    def __init__(self, k, hidden, max_user=max_user, max_item=max_item):\n",
    "        super(MLPMatrixFactorization, self).__init__(prefix='MLP_MF_')\n",
    "        \n",
    "        # user feature lookup\n",
    "        with self.name_scope():\n",
    "            self.user_embedding = gluon.nn.Embedding(input_dim=max_user, output_dim = k, prefix='emb_user_') \n",
    "            self.user_mlp = gluon.nn.Dense(hidden, prefix='dense_user_')\n",
    "\n",
    "            # item feature lookup\n",
    "            self.item_embedding = gluon.nn.Embedding(input_dim=max_item, output_dim = k, prefix='emb_item_') \n",
    "            self.item_mlp = gluon.nn.Dense(hidden, prefix='dense_item_')\n",
    "    \n",
    "    def hybrid_forward(self, F, user, item):\n",
    "        user_embeddings = self.user_embedding(user)\n",
    "        user_embeddings_relu = user_embeddings.relu()\n",
    "        user_transformed = self.user_mlp(user_embeddings_relu)\n",
    "        \n",
    "        items_embeddings = self.item_embedding(item)\n",
    "        items_embeddings_relu = items_embeddings.relu()\n",
    "        items_transformed = self.item_mlp(items_embeddings_relu)\n",
    "        \n",
    "        # predict by the inner product, which is elementwise product and then sum\n",
    "        pred = (user_transformed * items_transformed).sum(axis=1)\n",
    "        \n",
    "        return pred.flatten()\n",
    "\n",
    "net2 = MLPMatrixFactorization(64, 64)\n",
    "net2.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "mx.viz.plot_network(net2(mx.sym.var('user'), mx.sym.var('item')), node_attrs={\"fixedsize\":\"false\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2.summary(user.as_in_context(ctx[0]), item.as_in_context(ctx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses_2 = train(net2, train_data, test_data, epochs=15, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try training with the Adam optimizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = MLPMatrixFactorization(64, 64)\n",
    "net2.initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_2_adam  = train(net2, train_data, test_data, epochs=15, optimizer='adam', learning_rate=0.01, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network (Residual Network / ResNet)\n",
    "Borrowing ideas from [Deep Residual Learning for Image Recognition (He, et al.)](https://arxiv.org/abs/1512.03385) to build a complex deep network that is aggressively regularized, thanks to the dropout layers, to avoid over-fitting, but still achieves good performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_block(prefix='res_block_', hidden=64):\n",
    "    block = gluon.nn.HybridSequential(prefix=prefix)\n",
    "    with block.name_scope():\n",
    "        block.add(\n",
    "            gluon.nn.Dense(hidden, activation='relu', prefix='d1_'),\n",
    "            gluon.nn.Dropout(0.5, prefix='dropout_'),\n",
    "            gluon.nn.Dense(hidden, prefix='d2_')\n",
    "        )\n",
    "    return block\n",
    "    \n",
    "class ResNetMatrixFactorization(gluon.HybridBlock):\n",
    "    \n",
    "    def __init__(self, k, hidden, max_user=max_user, max_item=max_item):\n",
    "        super(ResNetMatrixFactorization, self).__init__(prefix='ResNet_MF_')\n",
    "        \n",
    "        # user feature lookup\n",
    "        with self.name_scope():\n",
    "            self.user_embedding = gluon.nn.Embedding(input_dim=max_user, output_dim = k, prefix='emb_user_')\n",
    "            self.user_block1 = get_residual_block('u_block1_', hidden)\n",
    "            self.user_dropout = gluon.nn.Dropout(0.5)\n",
    "            self.user_block2 = get_residual_block('u_block2_', hidden)           \n",
    "            \n",
    "            # item feature lookup\n",
    "            self.item_embedding = gluon.nn.Embedding(input_dim=max_item, output_dim = k, prefix='emb_item_')\n",
    "            self.item_block1 = get_residual_block('i_block1_', hidden)\n",
    "            self.item_dropout = gluon.nn.Dropout(0.5)\n",
    "            self.item_block2 = get_residual_block('i_block2_', hidden)           \n",
    "            \n",
    "    \n",
    "    def hybrid_forward(self, F, user, item):\n",
    "        user_embeddings = self.user_embedding(user)\n",
    "        user_block1 = self.user_block1(user_embeddings)\n",
    "        user1 = (user_embeddings + user_block1).relu()\n",
    "        \n",
    "        user2 = self.user_dropout(user1)\n",
    "        user_block2 = self.user_block2(user2)\n",
    "        user_transformed = (user2 + user_block2).relu()\n",
    "        \n",
    "        item_embeddings = self.item_embedding(item)\n",
    "        item_block1 = self.item_block1(item_embeddings)\n",
    "        item1 = (item_embeddings + item_block1).relu()\n",
    "        \n",
    "        item2 = self.item_dropout(item1)\n",
    "        item_block2 = self.item_block2(item2)\n",
    "        item_transformed = (item2 + item_block2).relu()\n",
    "        \n",
    "        # predict by the inner product, which is elementwise product and then sum\n",
    "        pred = (user_transformed * item_transformed).sum(axis=1)\n",
    "        \n",
    "        return pred.flatten()\n",
    "\n",
    "net3 = ResNetMatrixFactorization(128, 128)\n",
    "net3.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "mx.viz.plot_network(net3(mx.sym.var('user'), mx.sym.var('item')), node_attrs={\"fixedsize\":\"false\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3.summary(user.as_in_context(ctx[0]), item.as_in_context(ctx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_3  = train(net3, train_data, test_data, epochs=15, optimizer='adam', learning_rate=0.001, ctx=ctx, num_epoch_lr=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to the linear model where we can use directly the embedding weights, here we compute each combination of user / items and store predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "users = []\n",
    "items = []\n",
    "for i in range(max_user):\n",
    "    for j in range(max_item):\n",
    "        users.append(i+1)\n",
    "        items.append(j+1)\n",
    "dataset = gluon.data.ArrayDataset(np.array(users).astype('float32'), np.array(items).astype('float32'))\n",
    "dataloader = gluon.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "ratings = np.zeros((max_user+1, max_item+1))\n",
    "for users, items in dataloader:\n",
    "    users = users.as_in_context(ctx[0])\n",
    "    items = items.as_in_context(ctx[0])\n",
    "    scores = net3(users, items).asnumpy()\n",
    "    ratings[users.asnumpy().astype('int32'), items.asnumpy().astype('int32')] = scores.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_embeddings(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training\n",
    "Now let's draw a single chart that compares the learning curves of the two different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1,  test_1  = list(zip(*losses_1))\n",
    "train_1a, test_1a = list(zip(*losses_1_adam))\n",
    "train_2,  test_2  = list(zip(*losses_2))\n",
    "train_2a, test_2a = list(zip(*losses_2_adam))\n",
    "train_3a, test_3a = list(zip(*losses_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Evolution of training and testing losses')\n",
    "x = range(15)\n",
    "h1,  = plt.plot(x, test_1, 'c', label='test loss Linear')\n",
    "h2,  = plt.plot(x, train_1, 'c--', label='train loss Linear')\n",
    "h3,  = plt.plot(x, test_1a, 'b', label='test loss Linear Adam')\n",
    "h4,  = plt.plot(x, train_1a, 'b--', label='train loss Linear Adam')\n",
    "h5,  = plt.plot(x, test_2, 'r', label='test loss MLP')\n",
    "h6,  = plt.plot(x, train_2, 'r--', label='train loss MLP')\n",
    "h7,  = plt.plot(x, test_2a, 'm', label='test loss MLP Adam')\n",
    "h8,  = plt.plot(x, train_2a, 'm--', label='train loss MLP Adam')\n",
    "h9,  = plt.plot(x, test_3a, 'g', label='test loss ResNet Adam')\n",
    "h10, = plt.plot(x, train_3a, 'g--', label='train loss ResNet Adam')\n",
    "l   = plt.legend(handles=[h1, h2, h3, h4, h5, h6, h7, h8, h9, h10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "This tutorial is inspired by some examples from [xlvector/github](https://github.com/xlvector/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
